                                                                     |
                                                                     |-A1b3: LR annealing from 2.5E-5 to 1E-6
                                                                     |
                                                                     |-A1b2: LR = 2.5E-6
                                                                     |
                                                                     |-A1b1: LR = 2.5E-5-----A1b1a: LR = 2.5E-4, PER: a=0.6,b=1    
                                                                     |                     |
|---------------------A1b: Use the hyperparamters from DQN paper.-----                     |-A1b1b: LR = 2.5E-4, PER: a=0.8,b=1
|                     HIDDEN LAYER  = 250                                                  |
|                     BATCH_SIZE    = 32                                                   |-A1b1c: LR = 2.5E-4, PER: a=0.4,b=1 
|                     LR            = 2.5E-4
|                     EPSILON       = 0 to 0.995 with expoential decay
|                     LOSS          = HUBER LOSS
|                     OPTIMIZER     = RMS PROP
|                     TARGET UPDATE = 15 MONTHS
|
|
|
|                                                                    |-A1a2: LR = 2.5E-6
|                                                                    |
|                                                                    |-A1a1: LR = 2.5E-5    
|                                                                    |
|---------------------A1a: Use the hyperparamters from DQN paper.----
|                     HIDDEN LAYER  = 50
|                     BATCH_SIZE    = 32
|                     LR            = 2.5E-4
|                     EPSILON       = 0 to 0.995 with expoential decay
|                     LOSS          = HUBER LOSS
|                     OPTIMIZER     = RMS PROP
|                     TARGET UPDATE = 15 MONTHS
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|	****************************************************************
|	|---D0: More general reward function. DDQN with PER
|	|
|	|    C0: B0 with PER. Not very successful
|	|    |
|------ |   
|	|    B0: More general Reward function
|	|    |
|	|----A2: Dueling Double DQN
|	****************************************************************
|
A1 : Double DQN
|
A0: Careful Explorer. Actions are limited within a certain range of greedy action
|
|
A       : Vary Batch Size 
A0      : BATCH_SIZE = 12 - (10/10) - BEST RESULTS
A1      : BATCH_SIZE = 24 - (10/10)
A2 = S3 : BATCH_SIZE = 32 - (10/10)
A3      : BATCH_SIZE = 48 -  (8/10)
A4      : BATCH_SIZE = 64 -  (8/10)
|
|
********************************************************************
S3                                                                 *
********************************************************************
MODEL:                                                             *
S3 : INPUT->FC1->RELU->FC_OUT->OUTPUT                              *
LOSS : MSE                                                         *
                                                                   * 
LEARNING:   INIT_WEIGHT     = FC1 : KAIMING                        *
                              OUT : XAVIER                         *
            WIDTH           = 50                                   *
            DEPTH           = 1 + 1                                *
            WEIGHT_DECAY    = NONE                                 *
            LR              = 1e-4                                 *
            UPDATE_FREQ     = 18 MONTHS                            *
            MEMORY          = 36 MONTHS                            *
            ITERATION       = 100                                  *
            BATCH_SIZE      = 32                                   *
            EPSILON         = EPSILON SCHEDULING-[0.762 to 0.967]  *           
            GAMMA           = 0.9                                  *
            LAMBDA          = 0.95                                 *
                                                                   *
TRAINING:   TOKYO [2000-2009]                                      *
            SCHEDULING        = V3                                 *
            BATTERY_RESET     = 0%                                 *
            REWARD_FUNC       = 1d                                 *
            REWARD_BROADCAST  = TRUE                               *
            VIOLATION PENALTY = NONE                               *
                                                                   *
VALIDATION: TOKYO[2000-2018]                                       *
            GREEDY POLICY                                          *
********************************************************************
|
|
S3 - Scheduling V3, Reward Function 1d, No violations, Update = 18 months, Iterations = 100, 1xRELU,MSE, WT_DECAY=None, Memory = 3 years  -> (10/10)
|
|
Q3 - Scheduling V3, Reward Function 1d, No violations, Update = 18 months, Iterations = 100, 1xRELU,MSE, WT_DECAY=None                    -> (10/10)
|
|
I1 - Scheduling V3, Reward Function 1c, Update = 18 months, Iterations = 100, 1xRELU,MSE                                                  -> 10/10 (9/10) -1 BEST SEED:4,6,1,7
|
|
no_spark
